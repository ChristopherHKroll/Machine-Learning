{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f87df5f-951a-46ae-9543-86b06b37b7f8",
   "metadata": {},
   "source": [
    "## 8.15 Homework 8: Neural Networks and Deep Learning\n",
    "\n",
    "Note: it's very likely that in this homework your answers won't match the options exactly. That's okay and expected. Select the option that's closest to your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e37cb8f-7cec-49b2-9808-ddc9ae19e41d",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In this homework, we'll build a model for classifying various hair types. For this, we will use the Hair Type dataset that was obtained from Kaggle and slightly rebuilt: \n",
    "\n",
    "    https://www.kaggle.com/datasets/kavyasreeb/hair-type-dataset\n",
    "\n",
    "You can download the target dataset for this homework from here: https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n",
    "\n",
    "In the lectures we saw how to use a pre-trained neural network. In the homework, we'll train a much smaller model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea52de0-4064-4197-b28d-787148499172",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "The dataset contains around 1000 images of hairs in the separate folders for training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b576d-7792-4994-b7d2-a52cb652e9b9",
   "metadata": {},
   "source": [
    "### Reproducibility\n",
    "\n",
    "Reproducibility in deep learning is a multifaceted challenge that requires attention to both software and hardware details. In some cases, we can't guarantee exactly the same results during the same experiment runs. Therefore, in this homework we suggest to:\n",
    "\n",
    "- install tensorflow version 2.17.1\n",
    "- set the seed generators by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d22b6020-f9a7-4abe-b68f-e01dbece4c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2608f808-88fc-48d1-833e-1e58a5967a33",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "For this homework we will use Convolutional Neural Network (CNN). Like in the lectures, we'll use Keras.\n",
    "\n",
    "You need to develop the model with following structure:\n",
    "- The shape for input should be (200, 200, 3)\n",
    "- Next, create a convolutional layer (Conv2D):\n",
    "    - Use 32 filters\n",
    "    - Kernel size should be (3, 3) (that's the size of the filter)\n",
    "    - Use 'relu' as activation\n",
    "- Reduce the size of the feature map with max pooling (MaxPooling2D)\n",
    "    - Set the pooling size to (2, 2)\n",
    "- Turn the multi-dimensional result into vectors using a Flatten layer\n",
    "- Next, add a Dense layer with 64 neurons and 'relu' activation\n",
    "- Finally, create the Dense layer with 1 neuron - this will be the output\n",
    "    - The output layer should have an activation - use the appropriate activation for the binary classification case\n",
    "\n",
    "As optimizer use SGD with the following parameters:\n",
    "- SGD(lr=0.002, momentum=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76a53d-afce-4191-b006-0fe5828491c9",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Since we have a binary classification problem, what is the best loss function for us?\n",
    "- mean squared error\n",
    "- binary crossentropy\n",
    "- categorical crossentropy\n",
    "- cosine similarity\n",
    "\n",
    "Note: since we specify an activation for the output layer, we don't need to set from_logits=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e71deb-5b73-42f7-88ee-ce8132da75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd73520b-9881-473a-a619-8aac48459c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313632</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │    <span style=\"color: #00af00; text-decoration-color: #00af00\">20,072,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313632\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │    \u001b[38;5;34m20,072,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,073,473</span> (76.57 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,073,473\u001b[0m (76.57 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,073,473</span> (76.57 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,073,473\u001b[0m (76.57 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "def create_cnn_model():\n",
    "    model = Sequential([\n",
    "        # Explicit Input layer\n",
    "        Input(shape=(200, 200, 3)),\n",
    "        # Convolutional layer\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "        # MaxPooling layer\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Flatten layer\n",
    "        Flatten(),\n",
    "        # Fully connected Dense layer\n",
    "        Dense(64, activation='relu'),\n",
    "        # Output Dense layer for binary classification\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_cnn_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb656e-f7de-4263-884e-5624cbeacfc6",
   "metadata": {},
   "source": [
    "#### Question 1 - Answer: binary crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e44c66e-e7e6-4334-8d32-1d914140a539",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What's the total number of parameters of the model? You can use the summary method for that.\n",
    "- 896\n",
    "- 11214912\n",
    "- 15896912\n",
    "- 20072512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b818f607-9a2b-4719-8b1d-9711195cacea",
   "metadata": {},
   "source": [
    "#### Question 2 - Answer: 20072512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a10db-594c-487f-b378-747b967ab164",
   "metadata": {},
   "source": [
    "### Generators and Training\n",
    "For the next two questions, use the following data generator for both train and test sets:\n",
    "\n",
    "    ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "- We don't need to do any additional pre-processing for the images.\n",
    "- When reading the data from train/test directories, check the class_mode parameter. Which value should it be for a binary classification problem?\n",
    "- Use batch_size=20\n",
    "- Use shuffle=True for both training and test sets.\n",
    "\n",
    "For training use .fit() with the following params:\n",
    "\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=10,\n",
    "        validation_data=test_generator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "374987f0-acad-476a-83ff-a1a07e819ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c229872b-997e-4478-aada-7e3840918b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n",
      "Found 201 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christopher\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 300ms/step - accuracy: 0.5259 - loss: 0.8649 - val_accuracy: 0.6119 - val_loss: 0.6928\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 295ms/step - accuracy: 0.5610 - loss: 0.6926 - val_accuracy: 0.5821 - val_loss: 0.6919\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 305ms/step - accuracy: 0.6296 - loss: 0.6904 - val_accuracy: 0.6418 - val_loss: 0.6880\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 298ms/step - accuracy: 0.6290 - loss: 0.6848 - val_accuracy: 0.5323 - val_loss: 0.6809\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 289ms/step - accuracy: 0.6118 - loss: 0.6680 - val_accuracy: 0.5821 - val_loss: 0.6607\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 291ms/step - accuracy: 0.6838 - loss: 0.6263 - val_accuracy: 0.6418 - val_loss: 0.6278\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 293ms/step - accuracy: 0.6548 - loss: 0.6221 - val_accuracy: 0.6318 - val_loss: 0.6333\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 293ms/step - accuracy: 0.6870 - loss: 0.5948 - val_accuracy: 0.5622 - val_loss: 0.7153\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 290ms/step - accuracy: 0.6917 - loss: 0.5733 - val_accuracy: 0.6368 - val_loss: 0.6223\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 299ms/step - accuracy: 0.7206 - loss: 0.5435 - val_accuracy: 0.6617 - val_loss: 0.6043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x242ad113c90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load train and test data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    './data/train/',\n",
    "    target_size=(200, 200),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    './data/test/',\n",
    "    target_size=(200, 200),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68047187-9bc8-4bbb-8619-14e57fd147f6",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What is the median of training accuracy for all the epochs for this model?\n",
    "- 0.10\n",
    "- 0.32\n",
    "- 0.50\n",
    "- 0.72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dda9bd-0413-4177-9314-43c575861bb9",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What is the standard deviation of training loss for all the epochs for this model?\n",
    "- 0.028\n",
    "- 0.068\n",
    "- 0.128\n",
    "- 0.168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60abe68d-2a0d-4864-9681-41d8a18cc83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 302ms/step - accuracy: 0.7202 - loss: 0.5488 - val_accuracy: 0.6667 - val_loss: 0.5998\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 292ms/step - accuracy: 0.7156 - loss: 0.5452 - val_accuracy: 0.6816 - val_loss: 0.6026\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 293ms/step - accuracy: 0.6962 - loss: 0.5762 - val_accuracy: 0.6866 - val_loss: 0.5983\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 295ms/step - accuracy: 0.7512 - loss: 0.5094 - val_accuracy: 0.6667 - val_loss: 0.6024\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 303ms/step - accuracy: 0.7631 - loss: 0.4932 - val_accuracy: 0.6866 - val_loss: 0.5856\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 294ms/step - accuracy: 0.7630 - loss: 0.4798 - val_accuracy: 0.6866 - val_loss: 0.5740\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 289ms/step - accuracy: 0.7814 - loss: 0.4752 - val_accuracy: 0.6965 - val_loss: 0.5690\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 289ms/step - accuracy: 0.8021 - loss: 0.4337 - val_accuracy: 0.6816 - val_loss: 0.5685\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 292ms/step - accuracy: 0.8063 - loss: 0.4391 - val_accuracy: 0.6766 - val_loss: 0.5576\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 305ms/step - accuracy: 0.8002 - loss: 0.4354 - val_accuracy: 0.6816 - val_loss: 0.5803\n",
      "Median Training Accuracy: 0.7643750011920929\n",
      "Standard Deviation of Training Loss: 0.03854263977277474\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator\n",
    ")\n",
    "\n",
    "# Access training accuracy and loss for all epochs\n",
    "training_accuracies = history.history['accuracy']\n",
    "training_losses = history.history['loss']\n",
    "\n",
    "# Calculate the median of the training accuracies\n",
    "median_accuracy = np.median(training_accuracies)\n",
    "\n",
    "# Calculate the standard deviation of the training losses\n",
    "std_dev_loss = np.std(training_losses)\n",
    "\n",
    "# Print results\n",
    "print(f\"Median Training Accuracy: {median_accuracy}\")\n",
    "print(f\"Standard Deviation of Training Loss: {std_dev_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010da8d-07d6-4952-85b5-daa8caf8c90f",
   "metadata": {},
   "source": [
    "#### Question 3 - Answer: 0.72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c7e069-778a-48f1-bddb-de3a1a5f6ae0",
   "metadata": {},
   "source": [
    "#### Question 4 - Answer: 0.028"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb7ee7c-6267-49bc-af38-922afa627daf",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "For the next two questions, we'll generate more data using data augmentations.\n",
    "\n",
    "Add the following augmentations to your training data generator:\n",
    "- rotation_range=50,\n",
    "- width_shift_range=0.1,\n",
    "- height_shift_range=0.1,\n",
    "- zoom_range=0.1,\n",
    "- horizontal_flip=True,\n",
    "- fill_mode='nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2888f047-3583-4d73-908e-f7f605358cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n",
      "Found 201 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Augmented training data generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,          # Normalize pixel values to [0, 1]\n",
    "    rotation_range=50,       # Randomly rotate images by up to 50 degrees\n",
    "    width_shift_range=0.1,   # Shift images horizontally by up to 10% of width\n",
    "    height_shift_range=0.1,  # Shift images vertically by up to 10% of height\n",
    "    zoom_range=0.1,          # Randomly zoom in/out by up to 10%\n",
    "    horizontal_flip=True,    # Randomly flip images horizontally\n",
    "    fill_mode='nearest'      # Fill empty pixels with the nearest value\n",
    ")\n",
    "\n",
    "# Test data generator (no augmentation, only rescaling)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load train data with augmentations\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    './data/train/',\n",
    "    target_size=(200, 200),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Load test data without augmentations\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    './data/test/',\n",
    "    target_size=(200, 200),\n",
    "    batch_size=20,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f7a7a-9d99-4b98-8496-7bb9d668592f",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Let's train our model for 10 more epochs using the same code as previously.\n",
    "\n",
    "    Note: make sure you don't re-create the model - we want to continue training the model we already started training.\n",
    "\n",
    "What is the mean of test loss for all the epochs for the model trained with augmentations?\n",
    "- 0.26\n",
    "- 0.56\n",
    "- 0.86\n",
    "- 1.16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32282d26-6690-47a9-842b-bca554149411",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What's the average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations?\n",
    "- 0.31\n",
    "- 0.51\n",
    "- 0.71\n",
    "- 0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a632ec9-4252-4211-91e7-a63d6e60ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('model_with_augmentations.keras')\n",
    "\n",
    "# Later, load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model_with_augmentations.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "738a2216-793a-437b-854a-9733f70b797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 414ms/step - accuracy: 0.6729 - loss: 0.6015 - val_accuracy: 0.7413 - val_loss: 0.5103\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 409ms/step - accuracy: 0.6987 - loss: 0.5611 - val_accuracy: 0.7313 - val_loss: 0.5009\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 405ms/step - accuracy: 0.7276 - loss: 0.5389 - val_accuracy: 0.7463 - val_loss: 0.5281\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 415ms/step - accuracy: 0.7021 - loss: 0.5560 - val_accuracy: 0.7463 - val_loss: 0.5135\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 414ms/step - accuracy: 0.7582 - loss: 0.5268 - val_accuracy: 0.7363 - val_loss: 0.5074\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 410ms/step - accuracy: 0.7183 - loss: 0.5518 - val_accuracy: 0.7612 - val_loss: 0.4980\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 412ms/step - accuracy: 0.7429 - loss: 0.5291 - val_accuracy: 0.7264 - val_loss: 0.5211\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 411ms/step - accuracy: 0.6891 - loss: 0.5623 - val_accuracy: 0.7065 - val_loss: 0.5789\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 411ms/step - accuracy: 0.7199 - loss: 0.5421 - val_accuracy: 0.7264 - val_loss: 0.5457\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 405ms/step - accuracy: 0.7234 - loss: 0.5487 - val_accuracy: 0.7612 - val_loss: 0.4937\n",
      "Mean Test Loss: 0.5197592884302139\n",
      "Average Test Accuracy (Last 5 Epochs): 0.7363184332847595\n"
     ]
    }
   ],
   "source": [
    "# Continue training the same model\n",
    "history_augmented = model.fit(\n",
    "    train_generator,          # Augmented training data generator\n",
    "    epochs=10,                # Additional epochs\n",
    "    validation_data=test_generator  # Test data\n",
    ")\n",
    "\n",
    "# Access test loss for all epochs\n",
    "test_losses = history_augmented.history['val_loss']\n",
    "\n",
    "# Calculate the mean of test losses\n",
    "mean_test_loss = np.mean(test_losses)\n",
    "\n",
    "# Access test accuracy for all epochs\n",
    "test_accuracies = history_augmented.history['val_accuracy']\n",
    "\n",
    "# Calculate the average test accuracy for the last 5 epochs\n",
    "average_test_accuracy_last_5 = np.mean(test_accuracies[-5:])\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Test Loss: {mean_test_loss}\")\n",
    "print(f\"Average Test Accuracy (Last 5 Epochs): {average_test_accuracy_last_5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5abe5b2-3d6e-4d21-b327-ed89df037caf",
   "metadata": {},
   "source": [
    "#### Question 5 - Answer: 0.56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d085f96b-7adc-4f1a-b71a-d7081fa73bf5",
   "metadata": {},
   "source": [
    "#### Question 6 - Answer: 0.71"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
